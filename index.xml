<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Raghav Chawla</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Raghav Chawla</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>/img/icon-192.png</url>
      <title>Raghav Chawla</title>
      <link>/</link>
    </image>
    
    <item>
      <title> Google Sideways: My two cents</title>
      <link>/post/google_sideways/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/google_sideways/</guid>
      <description>&lt;p&gt;This blog talks about a recent improvement in the backpropagation in neural networks and was introduced by Google Deepmind in 2020. The paper being referred to is “Sideways: Depth-Parallel Training of Video Models” and can be found here.&lt;/p&gt;

&lt;p&gt;The basic idea is to use the time between the forward and backward passes of two consecutive inputs in a neural network, which was completely unused in the conventional backpropagation algorithm.&lt;/p&gt;

&lt;p&gt;While reading this paper, I realize how no one could think of this idea before? Just look at the below image and see how thoughtful and simple this idea is.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;Sideways_Intro_1.PNG&#34; alt=&#34;alt text&#34; title=&#34;Sideways: Introduction&#34;&gt;&lt;figcaption&gt;Sideways: Introduction&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;It’s okay if this image does not click in the first impression. Read on, and I will try to explain as much as I could understand.&lt;/p&gt;

&lt;p&gt;I am assuming that the reader is aware of the normal backpropagation algorithm used in neural networks. If not, a nice explanation can be found &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/chap2.html&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What innovation the sideways algorithm brings is that while one input is being propagated forward and then backward in order to update the weights, the next input is not being used. The practical networks either skip some of the input samples in this time frame or wait for both the forward and backward passes to complete. This is clearly a waste of time that could be used. The reason this time could not be used is that in the conventional backpropagation, during the backward pass, the change in weight of a node in any layer is calculated by the activation fn. (calculated during the forward pass) and the derivative of cost w.r.t. weight (calculated during the backward pass). See the below equation for clarity:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;BP_original_equation.PNG&#34; alt=&#34;alt text&#34; title=&#34;Backpropagation Update equation&#34;&gt;&lt;figcaption&gt;Backpropagation Update equation&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now, what sideways inherently assumes is that in any time series continuous data, some consecutive frames remain the same. Thus, the activation fn. due to these inputs will be approximately the same. So, we tweak our conventional backpropagation such that at every time sample, the new input is taken into the first layer and all the layers are propagated forward. On the side, the output cost fn. is calculated on the input which was N samples earlier (N being the no. of layers). Refer to the below image:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;Sideways_Complete_1.PNG&#34; alt=&#34;alt text&#34; title=&#34;Sideways Weights Update Method&#34;&gt;&lt;figcaption&gt;Sideways Weights Update Method&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now, for calculating any weight update, we see whichever latest backpropagated error is available at that time and multiply with the latest activation fn. Of the forward pass (Thus, the backpropagated error and the forward activation function are essentially of different input samples). The reason this is similar to the original backpropagation is that the input frames are continuous and thus, the activation functions and backpropagated errors won’t change much (except the frames near a change in the scene which can be ignored since that no. would be very small compared to the no. of total frames).&lt;/p&gt;

&lt;p&gt;“Okay, so using sideways proves to be time-saving. But, is there any other advantage of why I should be using this?”&lt;/p&gt;

&lt;p&gt;Well, yes. There is another improvement that we get which we may not have thought of while deploying sideways. When we keep on changing the inputs for updating the weights, the weights are not necessarily learning the feature of any particular input (since no weight update is dependent on any particular input, as opposed to conventional backpropagation which had every weight update to minimize the cost of every input). Thus, we are preventing our model to learn the intricacies of any particular input frames and thus, learn the overall features of the time series data. Or, to say in ML Lingo, we are introducing some sort of regularization in our model and thus, reducing overfitting. This effect is reflected in the testing accuracy graphs as below:&lt;/p&gt;

&lt;p&gt;“These seem to be all good flowers. But there has to be a thorn in it. Is there any hidden drawback that we could have been missing?”&lt;/p&gt;

&lt;p&gt;Yes, if we look closely, there are some drawbacks that may hinder the use of sideways in practical applications.&lt;/p&gt;

&lt;p&gt;For one, the loss function is pretty unstable, as seen in the following graph:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;BP_Sideways_Loss_Comparison.PNG&#34; alt=&#34;alt text&#34; title=&#34;Loss Function: Backpropagation vs Sideways&#34;&gt;&lt;figcaption&gt;Loss Function: Backpropagation vs Sideways&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The reason for this instability is obviously that we are not calculating the loss function in the manner it should be. The minute changes in a frame to the next will cause some irregularity between the activation function and the error derivative to minimize the cost function.&lt;/p&gt;

&lt;p&gt;The reason this instability is a problem for us is that though the accuracy has improved (calculated by the no. of correct predictions), we have not considered the confidence in case of wrong predictions. According to this graph, the algorithm predicts with high confidence even if the prediction is wrong. However, in practical applications, we will prefer that the network should not be confident (i.e., the prediction probability should not be way above the threshold if the prediction is wrong).&lt;/p&gt;

&lt;p&gt;Second, The paper considers that conventional backpropagation has some sort of overfitting which is being regularized to some extent by the sideways. But, if we are able to reduce this overfitting in BP by techniques such as dropout and regularization (though this may not be practically possible, just considering the case here), will the sideways be able to match the BP accuracy since the weight updates in Sideways are noisy?&lt;/p&gt;

&lt;p&gt;Third, the Sideways is applicable for only time-series data. How can we modify the algorithm to make it work for non-time-series data as well? In hindsight, we just need to introduce some correlation between the consecutive activation functions so that we can use a similar concept. There can be multiple ways that can be tested such as taking average of the last two/three inputs while calculating the activation function (and similarly, averaging last derivatives during the backward pass).&lt;/p&gt;

&lt;p&gt;All in all, I think that this is a great concept for improving speed and accuracy. The improvement figures reported in the paper are groundbreaking. Since there will be more hardware resources required for implementing Sideways, this opens up for a lot of research in architectural implementation to increase resource sharing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[Submitted] Timing Calibration in Interleaved Current Steering DACs</title>
      <link>/publication/conference-paper-2019-ieee-international-symposium-on-circuits-and-systems-iscas/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper-2019-ieee-international-symposium-on-circuits-and-systems-iscas/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>A year at intel</title>
      <link>/post/year-at-intel/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/year-at-intel/</guid>
      <description>&lt;p&gt;I joined Intel in June 2018 and it feels just like yesterday. This journey has been full of technical learning and professional development.&lt;/p&gt;

&lt;p&gt;I am working in the Physical Design team of the Image Processing Unit here, on meeting partition constraints of power, frequency, and area. This involves developing new algorithms for improving the quality and speed of convergence.&lt;/p&gt;

&lt;p&gt;I work under some amazing mentors who motivate me to challenge myself time and again. My colleagues share my enthusiasm and are always there to help me with my ideas. Working till late in the evening and brainstorming sessions are the best parts of my day.&lt;/p&gt;

&lt;p&gt;Being my first employer, Intel has helped me to get the gist of what is actually done in various teams across the Industry, a perspective far from what I had in college about the Industry.&lt;/p&gt;

&lt;p&gt;Although the so-called work-life balance depends upon what stage the project is in. Towards the end, this sometimes involves spending the whole night in the office, which I found surprisingly interesting, though others who have family or have been through the process several times would likely disagree.&lt;/p&gt;

&lt;p&gt;Going on team outings is definitely one of the major events I look up to. Sharing the common love for food, we have been to almost every buffet in Bangalore for team lunches. The recent potluck lunches and weekend sports have certainly played their part in bringing us together as a team.&lt;/p&gt;

&lt;p&gt;Recently, the management has started focusing more on &lt;em&gt;Product Output&lt;/em&gt; rather than &lt;em&gt;Individual Growth&lt;/em&gt;. Though both of these targets should go hand in hand, we are always stuffed with more projects of the same kind. I am always curious to know more but working on the same concepts repetitively becomes mundane.  Though there is a fair amount of office politics, nothing unusual in a corporate, but this can be avoided with fairly minimal effort.&lt;/p&gt;

&lt;p&gt;All in all, at Intel, I find a culture conducive for growth and learning. Whether I wish to work from home or leave the office early, there are no questions asked as long as I do justice with the work. The schedule becomes hectic sporadically, but we have always worked as a team in meeting deadlines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TSV Induced Stress Model and Its Application in Delay Estimation </title>
      <link>/publication/conference-paper-2018-ieee-soi-3d-subthreshold-microelectronics-technology-unified-conference-s3s/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper-2018-ieee-soi-3d-subthreshold-microelectronics-technology-unified-conference-s3s/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Cell Delay modeling for TSV induced stress in 3D ICs</title>
      <link>/project/delay-modeling-tsv/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/project/delay-modeling-tsv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Design of Analog Neuromorphic Circuits</title>
      <link>/project/nueromorphic/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/project/nueromorphic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling the effect of variability on the timing response of CMOS inverter-transmission gate structure</title>
      <link>/publication/conference-paper-2018-international-symposium-on-devices-circuits-and-systems-isdcs/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper-2018-international-symposium-on-devices-circuits-and-systems-isdcs/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Delay Modelling of a Static flip flop</title>
      <link>/project/delay-modeling-flip-flop/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/project/delay-modeling-flip-flop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Speed Limit</title>
      <link>/project/dynamic-speed-limit/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/project/dynamic-speed-limit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Propeller Clock</title>
      <link>/project/propeller-clock/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/project/propeller-clock/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Timing Calibration Algorithm for Interleaved Current Steering DAC</title>
      <link>/project/dac/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/project/dac/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Device Activity Recognition</title>
      <link>/project/activity-recognition/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/project/activity-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I-V characteristic variations due to defects in CMOS</title>
      <link>/project/defects-cmos/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/project/defects-cmos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Campus Buddy</title>
      <link>/project/campus-buddy/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      <guid>/project/campus-buddy/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
