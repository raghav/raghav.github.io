[{"authors":["admin"],"categories":null,"content":"I am a Graphics Hardware Engineer in the Image Processing Unit at Intel India, working on Graphics Architectures and Physical design optimization from synthesis to GDSII. Currently, I am working on a new algorithm to solve cell congestion hotspots in a design by suggesting alternate locations considering timing slack and DRCs.\nOn the side of fun, I like to cook for friends \u0026amp; family. Ever so often, I also like to write poems at YourQuote. Like every music wannabe, I bought a keyboard recently and am sluggishly learning to play it. On weekends, I can be found in a quiet corner at some literary event in Bangalore.\nI graduated from IIT Roorkee in 2018, where I was advised by Dr. Anand Bulusu. During my time at IIT Roorkee, I also had the pleasure to work with Dr. Sanjeev Manhas. For my hobby projects, I worked on some interesting stuff at the Artificial Intelligence \u0026amp; Electronics Section, IEEE Student Branch, and Mobile Development Group at IIT Roorkee.\nDuring my bachelor’s, I had the opportunity to explore many fields, ranging from Analog circuits, CMOS Devices to delay modeling in digital circuits. I also got to work on Industry research problems during my Internship at STMicroelectronics with Mr. Pratap Narayan Singh and during my major thesis at IIT Roorkee.\nI like interacting with my juniors. I did serve as a Teaching Assistant for the course ‘Semiconductor Devices’ twice and a mentor in the Institute’s Student mentorship program for freshmen undergrads.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589127507,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Graphics Hardware Engineer in the Image Processing Unit at Intel India, working on Graphics Architectures and Physical design optimization from synthesis to GDSII. Currently, I am working on a new algorithm to solve cell congestion hotspots in a design by suggesting alternate locations considering timing slack and DRCs.\nOn the side of fun, I like to cook for friends \u0026amp; family. Ever so often, I also like to write poems at YourQuote.","tags":null,"title":"Raghav Chawla","type":"authors"},{"authors":["Raghav Chawla"],"categories":null,"content":"Paper Referred: Sideways: Depth-Parallel Training of Video Models Authors: Mateusz Malinowski, Grzegorz Świrszcz, João Carreira, and Viorica Pătrăucean Why this research?\nThis blog talks about a recent improvement in the backpropagation in neural networks and was introduced by Google Deepmind in 2020.\nThe basic idea is to use the time between the forward and backward passes of two consecutive inputs in a neural network, which was completely unused in the conventional backpropagation algorithm.\nWhile reading this paper, I realize how no one could think of this idea before? Just look at the below image and see how thoughtful and simple this idea is.\nSideways: Introduction\nIt’s okay if this image does not click in the first impression. Read on, and I will try to explain as much as I could understand.\nThe Paper I am assuming that the reader is aware of the normal backpropagation algorithm used in neural networks. If not, a nice explanation can be found here.\nWhat innovation the sideways algorithm brings is that while one input is being propagated forward and then backward in order to update the weights, the next input is not being used. The practical networks either skip some of the input samples in this time frame or wait for both the forward and backward passes to complete. This is clearly a waste of time that could be used. The reason this time could not be used is that in the conventional backpropagation, during the backward pass, the change in weight of a node in any layer is calculated by the activation fn. (calculated during the forward pass) and the derivative of cost w.r.t. weight (calculated during the backward pass). See the below equation for clarity:\nBackpropagation Update equation\nNow, what sideways inherently assumes is that in any time series continuous data, some consecutive frames remain the same. Thus, the activation fn. due to these inputs will be approximately the same. So, we tweak our conventional backpropagation such that at every time sample, the new input is taken into the first layer and all the layers are propagated forward. On the side, the output cost fn. is calculated on the input which was N samples earlier (N being the no. of layers). Refer to the below image:\nSideways Weights Update Method\nNow, for calculating any weight update, we see whichever latest backpropagated error is available at that time and multiply with the latest activation fn. Of the forward pass (Thus, the backpropagated error and the forward activation function are essentially of different input samples). The reason this is similar to the original backpropagation is that the input frames are continuous and thus, the activation functions and backpropagated errors won’t change much (except the frames near a change in the scene which can be ignored since that no. would be very small compared to the no. of total frames).\n“Okay, so using sideways proves to be time-saving. But, is there any other advantage of why I should be using this?”\nWell, yes. There is another improvement that we get which we may not have thought of while deploying sideways. When we keep on changing the inputs for updating the weights, the weights are not necessarily learning the feature of any particular input (since no weight update is dependent on any particular input, as opposed to conventional backpropagation which had every weight update to minimize the cost of every input). Thus, we are preventing our model to learn the intricacies of any particular input frames and thus, learn the overall features of the time series data. Or, to say in ML Lingo, we are introducing some sort of regularization in our model and thus, reducing overfitting. This effect is reflected in the tranining accuracy graphs as below:\nTraining Accuracy: Backpropagation vs Sideways What Next? My two cents\n“These seem to be all good flowers. But there has to be a thorn in it. Is there any hidden drawback that we could have been missing?”\nYes, if we look closely, there are some drawbacks that may hinder the use of sideways in practical applications.\nFor one, the loss function is pretty unstable, as seen in the following graph:\nLoss Function: Backpropagation vs Sideways\nThe reason for this instability is obviously that we are not calculating the loss function in the manner it should be. The minute changes in a frame to the next will cause some irregularity between the activation function and the error derivative to minimize the cost function.\nThe reason this instability is a problem for us is that though the accuracy has improved (calculated by the no. of correct predictions), we have not considered the confidence in case of wrong predictions. According to this graph, the algorithm predicts with high confidence even if the prediction is wrong. However, in practical applications, we will prefer that the network should not be confident (i.e., the prediction probability should not be way above the threshold if the prediction is wrong).\nSecond, The paper considers that conventional backpropagation has some sort of overfitting which is being regularized to some extent by the sideways. But, if we are able to reduce this overfitting in BP by techniques such as dropout and regularization (though this may not be practically possible, just considering the case here), will the sideways be able to match the BP accuracy since the weight updates in Sideways are noisy?\nThird, the Sideways is applicable for only time-series data. How can we modify the algorithm to make it work for non-time-series data as well? In hindsight, we just need to introduce some correlation between the consecutive activation functions so that we can use a similar concept. There can be multiple ways that can be tested such as taking average of the last two/three inputs while calculating the activation function (and similarly, averaging last derivatives during the backward pass).\nAll in all, I think that this is a great concept for improving speed and accuracy. The improvement figures reported in the paper are groundbreaking. Since there will be more hardware resources required for implementing Sideways, this opens up for a lot of research in architectural implementation to increase resource sharing.\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589127507,"objectID":"9eb9dfe5dccb3156b9fd1f2f1bf47e58","permalink":"/post/google_sideways/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/google_sideways/","section":"post","summary":"Paper Referred: Sideways: Depth-Parallel Training of Video Models Authors: Mateusz Malinowski, Grzegorz Świrszcz, João Carreira, and Viorica Pătrăucean Why this research?\nThis blog talks about a recent improvement in the backpropagation in neural networks and was introduced by Google Deepmind in 2020.\nThe basic idea is to use the time between the forward and backward passes of two consecutive inputs in a neural network, which was completely unused in the conventional backpropagation algorithm.","tags":null,"title":" Google Sideways","type":"post"},{"authors":["Raghav Chawla"],"categories":null,"content":"Paper Referred: Compression with Multi-ECC: Enhanced Error Resiliency for Magnetic Memories Authors: Irina Alam, Saptadeep Pal and Puneet Gupta Why this research?\nWith the need for different properties (such as higher density, non-volatility, and higher performance) of memories, DRAM can’t be used for some applications. There are various candidates for replacing DRAMs such as PCM, STTRAM, and ReRAM. This research uses STTRAM because of zero leakage power and better endurance.\nSchematic of STT-RAM showing the anti-parallel and parallel states\nHowever, there are many issues associated with this memory such as a) With read current getting comparable to the write currents at lower nodes because of the unidirectional read operation, the read operation can alter the stored value. Since the read operation is asymmetric, this error comes when reading a 1 (i.e., it can switch 1 to 0), b) the time required for writing a data is lesser for writing a 0 as compared to writing a 1. Thus, write error occurs when current is removed before 1 has been written, c) Thermal instability of STTRAMs causes the stored data to flip. This stability can be increased by increasing the write current, thus reducing the write time. While the first 2 errors can be reduced by reducing the weight of hamming code (i.e., reducing the no. of 1s). the third can be reduced by increasing the robustness (by increasing the area).\nThe Paper This paper proposes a new method for detecting and correcting errors in code while reading and writing data into STTRAM. The compression in code is important in the sense that it leaves bits for error detection and corrections schemes. Better the compression, better is the error correction possible. The paper proposes Compression with Multi-ECC (CME) approach, using a Bit plane compression (modified for 64-bit words) which consists of delta-BitPlane-XoR to transform the data \u0026amp; encoding (run-length encoding combined with frequent pattern encoding) to compress the data, and using different types of correcting techniques according to the no. of bits in the compressed data. The proposed methodology copies the cache line only once and uses the duplicated data only if the original data has an error. The paper discusses the final cache line lengths based on the range in which compressed line length lies, padding 0s for the remaining bits.\nAn overview of the Bit-Plane Transformation scheme\nThen, the authors prove how adding 4 bits to the code can increase its robustness and use Dynamic Programming to choose the 6 final codes. Another set of bits called tag bits is required to denote the compression and ECC techniques used on the original code. The paper discusses two methods for storing tag bits: to store these bits separately in the memory which would require the data to wait till these bits arrive (otherwise, the decoding of compressed data would have started after the first burst of data) causing latency overhead and second, to include these bits in the compressed cache line itself, causing ECC to store the compressed data in 8 lesser bits.\nThe proposed methodology is evaluated against two design points in terms of write, read and retention error rates. The results show a reduction in hamming weight and thus, a reduction in block failure probability.\nComparsion of Average Hamming weight for different benchmarks\nIt is observed that the failure probability increases when the hamming code is inverted (even though the inverted code has less weight). This is because the inversion requires an extra bit per word, reducing the no. of bits available for error detection \u0026amp; correction schemes. This issue is handled by using a single bit for multiple inverted words and thus, saving the bits for ECC techniques.\nReduction in block error rate induced due to WER, RER and RDR\nThe paper then discusses the overheads due to ECC schemes: area overhead is not significant when compared to the processor size and energy overhead is \u0026lt; 1% of per-bit read energy. For the scheme with separately stored tag bits, the proposed ECC decoding has no latency overhead while it is 1 cycle for the scheme with tag bits embedded into the cache line. Since the priority first word (intended to improve performance) cannot be implemented, it causes more latency overhead in both the schemes.\nThe above methodology is compared with another compression scheme: BΔI. While this compression scheme reduces the hamming weight more efficiently than BPC and thus is expected to have lower error rates, BPC outperforms BΔI when the extra bits available after compression are used for error detection and correction methods. Then, the paper analyzes the reliability of using STTRAMs instead of DRAMs. While the STTRAM promises higher density and non-volatility over DRAMs, it requires ECC protection and has read, write and retention errors which can be much worse than transient bit errors in DRAM. But when the proposed protection scheme is used along with scrubbing, STTRAMs can be as reliable as the DRAMs.\nWhat Next? My two cents If the code represents an integer, MSBs of the integers can have better error detection and correction schemes than LSBs. This way, if the correction method is unable to correct the code, there will be a minimal deviation from the actual value. For example, if the total compressed length is 46 bits, the 17 MSBs (after compression) can be represented with 19 extra bits for 3EC4ED and 29 LSBs can be represented with 7 LSBs for SECDED. This will require separate compression for MSBs and LSBs. When compared with using two codes each of 23 data bits and 13 extra bits for DECTED method, this method gives more emphasis on correcting MSB errors. This way, we can use a slightly deviated output if the intended application can bear some inaccuracy in the integer output.\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589127507,"objectID":"8b37dd37e37d8d9b1491dae4c8558f55","permalink":"/post/compression_with_multi_ecc/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/compression_with_multi_ecc/","section":"post","summary":"Paper Referred: Compression with Multi-ECC: Enhanced Error Resiliency for Magnetic Memories Authors: Irina Alam, Saptadeep Pal and Puneet Gupta Why this research?\nWith the need for different properties (such as higher density, non-volatility, and higher performance) of memories, DRAM can’t be used for some applications. There are various candidates for replacing DRAMs such as PCM, STTRAM, and ReRAM. This research uses STTRAM because of zero leakage power and better endurance.","tags":null,"title":"Compression with Multi ECC Techniques","type":"post"},{"authors":["Raghav Chawla"],"categories":null,"content":"Paper Referred: Sparse Matrix to Matrix Multiplication: A Representation and Architecture for Acceleration Authors: Pareesa Ameneh Golnari, Sharad Malik Why this research? With the increasing demand of neural networks in new application areas and large no. of training features required for better accuracy, it is becoming increasingly important to reduce the memory required to store this data. Though the sparse matrix is an efficient way to store this data more efficiently, there is no efficient way for the sparse matrix to matrix multiplication. This paper proposes a new representation for sparse matrices and an architecture for their multiplication. A speedup of 14-49 times in accessing the data and of 9-30 times in multiplication is demonstrated.\nThe Paper The paper compares the random data access cost of various sparse formats with compressed row storage (CRS) being one of the most efficient formats. This paper proposes an Indexed CRS format to reduce the complexity of accessing a non-zero data. Each row of the matrix is divided into sections and each section is further divided into blocks. Each section is represented by a counter vector which contains no. of non-zero values before this section followed by no. of non-zero values in each of its blocks. Any values in this representation can be transformed into the corresponding matrix index (and vice-versa) using the information in section counter vectors. To locate an element at (i, j), the corresponding section no. is ⌊*j/S*⌋ and block no. is ⌊*(j%s)/b*⌋ where S and b are the lengths of the section and block respectively. Thus, for searching an element (i,j), we need to see only the corresponding block, reducing the random memory access to (*b/2 + 1*). Comparing this with the conventional CRS format, memory access reduces by a factor of *ND/(b+2)* and required storage increases by a factor of *(2DS+1)/(2DS)*, where N is the no. of rows and D is density (ratio of no. of non-zeros to dataset size). Next, this paper proposes a new architecture for implementing the sparse matrix multiplication (SpMM) represented in the format described above. In this direction, the paper discusses an algorithm for a dot product of the 1st matrix’s row a with 2nd matrix’s column b. Comparing elements of these two vectors, if the index of a’s ith element equals b’s jth element in the original matrices (found using section counter vector), the product of their values is added to the dot product and both operands are incremented. Else, only the vector with the smaller index is checked for the next element. While this algorithm serves its purpose, it suffers from a slower movement of operands because operand with larger index is not incremented and stalling where the inputs are shared across multiple nodes. To account for these drawbacks, a synchronized mesh architecture is proposed. Each node of the mesh has a comparator, a buffer and a flag located before the MAC unit. Unlike the previous algorithm, if the indices of two operands at hand do not match, the operand with a larger index is stored in the buffer and the corresponding flag is set, thus avoiding stalling of the data and using two operands per cycle. During the next comparison of indices, the flag is read to indicate if a value is stored in the buffer. If the operand with the smaller index can be matched with a buffered value, buffered value is used for multiplication. Synchronization is performed at the end of every R elements and buffers are erased to ensure that operand buffers are most R (depth of operand) element in size, thus preventing any stall. Next, the proposed sparse format and multiplication algorithm are evaluated against various datasets. It is observed that this architecture’s acceleration increases (i.e., latency reduces) as the density (i.e., the ratio of no. of non-zero elements with dataset size) reduces. This architecture is then compared with previous approaches for the given range of densities. The proposed SpMM architecture shows a 15-39 times improvement over conventional matrix multiplication and 2-30 times improvement over FPIC.  ","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589127507,"objectID":"aa1c51c03656c277153ba444c76fa950","permalink":"/post/sparse_matrix_multiplication/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/sparse_matrix_multiplication/","section":"post","summary":"Paper Referred: Sparse Matrix to Matrix Multiplication: A Representation and Architecture for Acceleration Authors: Pareesa Ameneh Golnari, Sharad Malik Why this research? With the increasing demand of neural networks in new application areas and large no. of training features required for better accuracy, it is becoming increasingly important to reduce the memory required to store this data. Though the sparse matrix is an efficient way to store this data more efficiently, there is no efficient way for the sparse matrix to matrix multiplication.","tags":null,"title":"Sparse Matrix Multiplication","type":"post"},{"authors":["Raghav Chawla","Pranshu Malik","P. N. Singh","A. Sharma"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573303949,"objectID":"dd5164eadacbae27398983847aaaa4e3","permalink":"/publication/conference-paper-2019-ieee-international-symposium-on-circuits-and-systems-iscas/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/conference-paper-2019-ieee-international-symposium-on-circuits-and-systems-iscas/","section":"publication","summary":"A feedback algorithm for removing timing mismatch problems and improving SNR of interleaved current steering DAC structures clocked at high frequency (in GHz) is proposed. The algorithm developed is tested to remove the timing errors at 4GHz frequency within a margin of 0.1% and improve the SNR by more than 24 dB and ENoB (Effective no. of  bits) by more than 4 bits","tags":null,"title":"[Submitted] Timing Calibration in Interleaved Current Steering DACs","type":"publication"},{"authors":["Raghav Chawla"],"categories":null,"content":"I joined Intel in June 2018 and it feels just like yesterday. This journey has been full of technical learning and professional development.\nI am working in the Physical Design team of the Image Processing Unit here, on meeting partition constraints of power, frequency, and area. This involves developing new algorithms for improving the quality and speed of convergence.\nI work under some amazing mentors who motivate me to challenge myself time and again. My colleagues share my enthusiasm and are always there to help me with my ideas. Working till late in the evening and brainstorming sessions are the best parts of my day.\nBeing my first employer, Intel has helped me to get the gist of what is actually done in various teams across the Industry, a perspective far from what I had in college about the Industry.\nAlthough the so-called work-life balance depends upon what stage the project is in. Towards the end, this sometimes involves spending the whole night in the office, which I found surprisingly interesting, though others who have family or have been through the process several times would likely disagree.\nGoing on team outings is definitely one of the major events I look up to. Sharing the common love for food, we have been to almost every buffet in Bangalore for team lunches. The recent potluck lunches and weekend sports have certainly played their part in bringing us together as a team.\nRecently, the management has started focusing more on Product Output rather than Individual Growth. Though both of these targets should go hand in hand, we are always stuffed with more projects of the same kind. I am always curious to know more but working on the same concepts repetitively becomes mundane. Though there is a fair amount of office politics, nothing unusual in a corporate, but this can be avoided with fairly minimal effort.\nAll in all, at Intel, I find a culture conducive for growth and learning. Whether I wish to work from home or leave the office early, there are no questions asked as long as I do justice with the work. The schedule becomes hectic sporadically, but we have always worked as a team in meeting deadlines.\n","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573572328,"objectID":"5f9bb0795400cf1494dc0f5fd42c96b4","permalink":"/post/year-at-intel/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/post/year-at-intel/","section":"post","summary":"I joined Intel in June 2018 and it feels just like yesterday. This journey has been full of technical learning and professional development.\nI am working in the Physical Design team of the Image Processing Unit here, on meeting partition constraints of power, frequency, and area. This involves developing new algorithms for improving the quality and speed of convergence.\nI work under some amazing mentors who motivate me to challenge myself time and again.","tags":null,"title":"A year at intel","type":"post"},{"authors":["Raghav Chawla","Sarita Yadav","Arvind Sharma","Baljit Kaur","Rajendra Pratap","Bulusu Anand"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573483345,"objectID":"aa0c1fa0dbc53997b9092535d85b22c1","permalink":"/publication/conference-paper-2018-ieee-soi-3d-subthreshold-microelectronics-technology-unified-conference-s3s/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/conference-paper-2018-ieee-soi-3d-subthreshold-microelectronics-technology-unified-conference-s3s/","section":"publication","summary":"In this paper, we developed models to calculate the variation of threshold voltage and mobility due to stress around a TSV in 3D ICs. These models were then used to calculate delay variation of an inverter around a TSV.","tags":null,"title":"TSV Induced Stress Model and Its Application in Delay Estimation ","type":"publication"},{"authors":null,"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"667df4c5c993d9ebc213f833fa689f8f","permalink":"/project/delay-modeling-tsv/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/delay-modeling-tsv/","section":"project","summary":"A holistic model to determine the threshold voltage and mobility changes around a TSV, and thus, delay through an inverter and a 2 input NAND gate. The final model needed coordinates of the cell to calculate the propagation delay.","tags":["Hardware"],"title":"Cell Delay modeling for TSV induced stress in 3D ICs","type":"project"},{"authors":null,"categories":null,"content":"","date":1525046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565808,"objectID":"e5fafc489dd049528e671a3ee43691b4","permalink":"/project/nueromorphic/","publishdate":"2018-04-30T00:00:00Z","relpermalink":"/project/nueromorphic/","section":"project","summary":"Designed the circuit of a Pulse-Based Analog Velocity Sensor. Pixels were designed to detect the edge of an object using contrasting lights. Multiple pixels were then used to measure the velocity of a moving object.","tags":["Hardware"],"title":"Design of Analog Neuromorphic Circuits","type":"project"},{"authors":["Arvind Sharma","Naushad Alam","Raghav Chawla","Bulusu Anand"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573303949,"objectID":"e73040949539c152f7396319482fd082","permalink":"/publication/conference-paper-2018-international-symposium-on-devices-circuits-and-systems-isdcs/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/conference-paper-2018-international-symposium-on-devices-circuits-and-systems-isdcs/","section":"publication","summary":"This paper presents a novel delay model for Inverter followed by Transmission Gate (Inv-Tx) structure. Our model is novel in the sense that it considers the series stack effect along with the internal node voltage and parasitic capacitances while treating the Inv-Tx structure as a single entity.","tags":null,"title":"Modeling the effect of variability on the timing response of CMOS inverter-transmission gate structure","type":"publication"},{"authors":null,"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"9f8f218d3d6d88f4b482082b40374912","permalink":"/project/delay-modeling-flip-flop/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/project/delay-modeling-flip-flop/","section":"project","summary":"A compact delay model to calculate delay through an Inverter-Transmission gate, considered as a single entity. Captured the effects of series stacking and parasitic capacitances while calculating delay.","tags":["Hardware"],"title":"Delay Modelling of a Static flip flop","type":"project"},{"authors":null,"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"75285700b7a13530dd74b1c3718c7b14","permalink":"/project/dynamic-speed-limit/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/project/dynamic-speed-limit/","section":"project","summary":"A solution to the long impending problem of Traffic jams in Indian metro cities. The algorithm used a Machine Learning model with the continuity equation to determine the ideal traffic speed on different roads. Developed a working prototype to demonstrate the algorithm.","tags":["Software"],"title":"Dynamic Speed Limit","type":"project"},{"authors":null,"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"892d4dcb625fabab8e876755621db4e2","permalink":"/project/propeller-clock/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/project/propeller-clock/","section":"project","summary":"Developed a clock (Analog and Digital) using a rotating PCB consisting of a linear array of LEDs, based on the principle of the Persistence of Vision. The LEDs were set to light up at fixed delays so as to show the time precisely.","tags":["Software","Hardware"],"title":"Propeller Clock","type":"project"},{"authors":null,"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"4b479e663de9f5eca3dda590b68e8fcd","permalink":"/project/dac/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/project/dac/","section":"project","summary":"A feedback algorithm for removing timing mismatch problems and improving SNR of interleaved current steering DAC structures clocked at high frequency (in GHz). The algorithm developed was tested to remove the timing errors at 4GHz frequency within a margin of 0.1% and improve the SNR by more than 24 dB and ENoB (Effective no. of  bits) by more than 4 bits.","tags":["Software","Hardware"],"title":"Timing Calibration Algorithm for Interleaved Current Steering DAC","type":"project"},{"authors":null,"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"2b317bf5df9d0ee4d71548ca48cb6e83","permalink":"/project/activity-recognition/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/activity-recognition/","section":"project","summary":"An Activity-based Smart health assistant that can provide health tips, product recommendations etc by analyzing daily activities such as walking, sitting, standing etc. Developed using Smartphone's accelerometer sensor, Machine learning (CNN).","tags":["Software"],"title":"On Device Activity Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"b02c5075a805fa88a36704347222166c","permalink":"/project/defects-cmos/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/project/defects-cmos/","section":"project","summary":"Studied different types of defects and the resulting shifting of bands. Simulated the corresponding changes in leakage current in MOSFET due to various forms of gate leakage and drain leakage.","tags":["Hardware"],"title":"I-V characteristic variations due to defects in CMOS","type":"project"},{"authors":null,"categories":null,"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573565163,"objectID":"dbd45cf6d8ae4c0a22130572e1e23d63","permalink":"/project/campus-buddy/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/project/campus-buddy/","section":"project","summary":"Campus Buddy is an assistant which keeps you updated with the events and activities of student groups and helps you search Telephone Directory of IIT Roorkee.","tags":["Software"],"title":"Campus Buddy","type":"project"}]